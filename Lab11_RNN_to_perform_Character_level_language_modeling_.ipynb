{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "UPGXgFSll6Su"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def initialize_parameters(vocab_size, hidden_layer_size):\n",
        "\n",
        "    parameters = {}\n",
        "    parameters[\"Whh\"] = np.random.randn(\n",
        "        hidden_layer_size, hidden_layer_size) * 0.01\n",
        "    parameters[\"Wxh\"] = np.random.randn(hidden_layer_size, vocab_size) * 0.01\n",
        "    parameters[\"b\"] = np.zeros((hidden_layer_size, 1))\n",
        "    parameters[\"Why\"] = np.random.randn(vocab_size, hidden_layer_size) * 0.01\n",
        "    parameters[\"c\"] = np.zeros((vocab_size, 1))\n",
        "\n",
        "    return parameters\n",
        "\n",
        "\n",
        "def initialize_adam(parameters):\n",
        "\n",
        "    parameters_names = [\"Whh\", \"Wxh\", \"b\", \"Why\", \"c\"]\n",
        "    v = {}\n",
        "    s = {}\n",
        "\n",
        "    for param_name in parameters_names:\n",
        "        v[\"d\" + param_name] = np.zeros_like(parameters[param_name])\n",
        "        s[\"d\" + param_name] = np.zeros_like(parameters[param_name])\n",
        "\n",
        "    return v, s\n",
        "\n",
        "\n",
        "def initialize_rmsprop(parameters):\n",
        "\n",
        "    parameters_names = [\"Whh\", \"Wxh\", \"b\", \"Why\", \"c\"]\n",
        "    s = {}\n",
        "\n",
        "    for param_name in parameters_names:\n",
        "        s[\"d\" + param_name] = np.zeros_like(parameters[param_name])\n",
        "\n",
        "    return s\n",
        "\n",
        "\n",
        "def softmax(z):\n",
        "\n",
        "    e_z = np.exp(z)\n",
        "    probs = e_z / np.sum(e_z)\n",
        "\n",
        "    return probs\n",
        "\n",
        "\n",
        "def rnn_forward(x, y, h_prev, parameters):\n",
        "\n",
        "    # Retrieve parameters\n",
        "    Wxh, Whh, b = parameters[\"Wxh\"], parameters[\"Whh\"], parameters[\"b\"]\n",
        "    Why, c = parameters[\"Why\"], parameters[\"c\"]\n",
        "\n",
        "    # Initialize inputs, hidden state, output, and probabilities dictionaries\n",
        "    xs, hs, os, probs = {}, {}, {}, {}\n",
        "\n",
        "    # Initialize x0 to zero vector\n",
        "    xs[0] = np.zeros((vocab_size, 1))\n",
        "\n",
        "    # Initialize loss and assigns h_prev to last hidden state in hs\n",
        "    loss = 0\n",
        "    hs[-1] = np.copy(h_prev)\n",
        "\n",
        "    # Forward pass: loop over all characters of the name\n",
        "    for t in range(len(x)):\n",
        "        # Convert to one-hot vector\n",
        "        if t > 0:\n",
        "            xs[t] = np.zeros((vocab_size, 1))\n",
        "            xs[t][x[t]] = 1\n",
        "        # Hidden state\n",
        "        hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t - 1]) + b)\n",
        "        # Logits\n",
        "        os[t] = np.dot(Why, hs[t]) + c\n",
        "        # Probs\n",
        "        probs[t] = softmax(os[t])\n",
        "        # Loss\n",
        "        loss -= np.log(probs[t][y[t], 0])\n",
        "\n",
        "    cache = (xs, hs, probs)\n",
        "\n",
        "    return loss, cache\n",
        "\n",
        "\n",
        "def smooth_loss(loss, current_loss):\n",
        "\n",
        "    return 0.999 * loss + 0.001 * current_loss\n",
        "\n",
        "\n",
        "def clip_gradients(gradients, max_value):\n",
        "\n",
        "    for grad in gradients.keys():\n",
        "        np.clip(gradients[grad], -max_value, max_value, out=gradients[grad])\n",
        "\n",
        "    return gradients\n",
        "\n",
        "\n",
        "def rnn_backward(y, parameters, cache):\n",
        "\n",
        "    # Retrieve xs, hs, and probs\n",
        "    xs, hs, probs = cache\n",
        "\n",
        "    # Initialize all gradients to zero\n",
        "    dh_next = np.zeros_like(hs[0])\n",
        "\n",
        "    parameters_names = [\"Whh\", \"Wxh\", \"b\", \"Why\", \"c\"]\n",
        "    grads = {}\n",
        "    for param_name in parameters_names:\n",
        "        grads[\"d\" + param_name] = np.zeros_like(parameters[param_name])\n",
        "\n",
        "    # Iterate over all time steps in reverse order starting from Tx\n",
        "    for t in reversed(range(len(xs))):\n",
        "        dy = np.copy(probs[t])\n",
        "        dy[y[t]] -= 1\n",
        "        grads[\"dWhy\"] += np.dot(dy, hs[t].T)\n",
        "        grads[\"dc\"] += dy\n",
        "        dh = np.dot(parameters[\"Why\"].T, dy) + dh_next\n",
        "        dhraw = (1 - hs[t] ** 2) * dh\n",
        "        grads[\"dWhh\"] += np.dot(dhraw, hs[t - 1].T)\n",
        "        grads[\"dWxh\"] += np.dot(dhraw, xs[t].T)\n",
        "        grads[\"db\"] += dhraw\n",
        "        dh_next = np.dot(parameters[\"Whh\"].T, dhraw)\n",
        "        # Clip the gradients using [-5, 5] as the interval\n",
        "        grads = clip_gradients(grads, 5)\n",
        "    # Get the last hidden state\n",
        "    h_prev = hs[len(xs) - 1]\n",
        "\n",
        "    return grads, h_prev\n",
        "\n",
        "\n",
        "def update_parameters_with_adam(\n",
        "        parameters, grads, v, s, t, learning_rate, beta1=0.9, beta2=0.999,\n",
        "        epsilon=1e-8):\n",
        "\n",
        "    parameters_names = [\"Whh\", \"Wxh\", \"b\", \"Why\", \"c\"]\n",
        "    v_corrected = {}\n",
        "    s_corrected = {}\n",
        "\n",
        "    for param_name in parameters_names:\n",
        "        # Update the moving average of first gradient and squared gradient\n",
        "        v[\"d\" + param_name] = beta1 * v[\"d\" + param_name] +\\\n",
        "            (1 - beta1) * grads[\"d\" + param_name]\n",
        "        s[\"d\" + param_name] = beta2 * s[\"d\" + param_name] +\\\n",
        "            (1 - beta2) * np.square(grads[\"d\" + param_name])\n",
        "\n",
        "        # Compute the corrected-bias estimate of the moving averages\n",
        "        v_corrected[\"d\" + param_name] = v[\"d\" + param_name] / (1 - beta1**t)\n",
        "        s_corrected[\"d\" + param_name] = s[\"d\" + param_name] / (1 - beta2**t)\n",
        "\n",
        "        # update parameters\n",
        "        parameters[param_name] -= (learning_rate *\n",
        "                                   v_corrected[\"d\" + param_name])\\\n",
        "            / (np.sqrt(s_corrected[\"d\" + param_name] + epsilon))\n",
        "\n",
        "    return parameters, v, s\n",
        "\n",
        "\n",
        "def update_parameters(parameters, grads, learning_rate):\n",
        "    for param in parameters.keys():\n",
        "        parameters[param] -= learning_rate * grads[\"d\" + param]\n",
        "\n",
        "    return parameters\n",
        "\n",
        "\n",
        "def update_parameters_with_rmsprop(\n",
        "        parameters, grads, s, beta=0.9, learning_rate=0.001, epsilon=1e-8):\n",
        "\n",
        "    parameters_names = [\"Whh\", \"Wxh\", \"b\", \"Why\", \"c\"]\n",
        "\n",
        "    for param_name in parameters_names:\n",
        "        # Update exponential weighted average of squared gradients\n",
        "        s[\"d\" + param_name] = beta * s[\"d\" + param_name] +\\\n",
        "            (1 - beta) * np.square(grads[\"d\" + param_name])\n",
        "\n",
        "        # Update parameters\n",
        "        parameters[param_name] -= (learning_rate * grads[\"d\" + param_name])\\\n",
        "            / (np.sqrt(s[\"d\" + param_name] + epsilon))\n",
        "\n",
        "    return parameters, s\n",
        "\n",
        "\n",
        "def sample(parameters, idx_to_chars, chars_to_idx, n):\n",
        "\n",
        "    # Retrienve parameters, shapes, and vocab size\n",
        "    Whh, Wxh, b = parameters[\"Whh\"], parameters[\"Wxh\"], parameters[\"b\"]\n",
        "    Why, c = parameters[\"Why\"], parameters[\"c\"]\n",
        "    n_h, n_x = Wxh.shape\n",
        "    vocab_size = c.shape[0]\n",
        "\n",
        "    # Initialize a0 and x1 to zero vectors\n",
        "    h_prev = np.zeros((n_h, 1))\n",
        "    x = np.zeros((n_x, 1))\n",
        "\n",
        "    # Initialize empty sequence\n",
        "    indices = []\n",
        "    idx = -1\n",
        "    counter = 0\n",
        "    while (counter <= n and idx != chars_to_idx[\"\\n\"]):\n",
        "        # Fwd propagation\n",
        "        h = np.tanh(np.dot(Whh, h_prev) + np.dot(Wxh, x) + b)\n",
        "        o = np.dot(Why, h) + c\n",
        "        probs = softmax(o)\n",
        "\n",
        "        # Sample the index of the character using generated probs distribution\n",
        "        idx = np.random.choice(vocab_size, p=probs.ravel())\n",
        "\n",
        "        # Get the character of the sampled index\n",
        "        char = idx_to_chars[idx]\n",
        "\n",
        "        # Add the char to the sequence\n",
        "        indices.append(idx)\n",
        "\n",
        "        # Update a_prev and x\n",
        "        h_prev = np.copy(h)\n",
        "        x = np.zeros((n_x, 1))\n",
        "        x[idx] = 1\n",
        "\n",
        "        counter += 1\n",
        "    sequence = \"\".join([idx_to_chars[idx] for idx in indices if idx != 0])\n",
        "\n",
        "    return sequence\n",
        "\n",
        "\n",
        "def model(\n",
        "        file_path, chars_to_idx, idx_to_chars, hidden_layer_size, vocab_size,\n",
        "        num_epochs=10, learning_rate=0.01):\n",
        "\n",
        "    # Get the data\n",
        "    with open(file_path) as f:\n",
        "        data = f.readlines()\n",
        "    examples = [x.lower().strip() for x in data]\n",
        "\n",
        "    # Initialize parameters\n",
        "    parameters = initialize_parameters(vocab_size, hidden_layer_size)\n",
        "\n",
        "    # Initialize Adam parameters\n",
        "    s = initialize_rmsprop(parameters)\n",
        "\n",
        "    # Initialize loss\n",
        "    smoothed_loss = -np.log(1 / vocab_size) * 7\n",
        "\n",
        "    # Initialize hidden state h0 and overall loss\n",
        "    h_prev = np.zeros((hidden_layer_size, 1))\n",
        "    overall_loss = []\n",
        "\n",
        "    # Iterate over number of epochs\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"\\033[1m\\033[94mEpoch {epoch}\")\n",
        "        print(f\"\\033[1m\\033[92m=======\")\n",
        "\n",
        "        # Sample one name\n",
        "        print(f\"\"\"Sampled name: {sample(parameters, idx_to_chars, chars_to_idx,\n",
        "            10).capitalize()}\"\"\")\n",
        "        print(f\"Smoothed loss: {smoothed_loss:.4f}\\n\")\n",
        "\n",
        "        # Shuffle examples\n",
        "        np.random.shuffle(examples)\n",
        "\n",
        "        # Iterate over all examples (SGD)\n",
        "        for example in examples:\n",
        "            x = [None] + [chars_to_idx[char] for char in example]\n",
        "            y = x[1:] + [chars_to_idx[\"\\n\"]]\n",
        "            # Fwd pass\n",
        "            loss, cache = rnn_forward(x, y, h_prev, parameters)\n",
        "            # Compute smooth loss\n",
        "            smoothed_loss = smooth_loss(smoothed_loss, loss)\n",
        "            # Bwd passA\n",
        "            grads, h_prev = rnn_backward(y, parameters, cache)\n",
        "            # Update parameters\n",
        "            parameters, s = update_parameters_with_rmsprop(\n",
        "                parameters, grads, s)\n",
        "\n",
        "        overall_loss.append(smoothed_loss)\n",
        "\n",
        "    return parameters, overall_loss"
      ],
      "metadata": {
        "id": "5KYHbQlv4Vd1"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "sLKRfg23NtBh"
      },
      "outputs": [],
      "source": [
        "def rnn_forward(x, y, h_prev, parameters):\n",
        "    Wxh, Whh, b = parameters[\"Wxh\"], parameters[\"Whh\"], parameters[\"b\"]\n",
        "    Why, c = parameters[\"Why\"], parameters[\"c\"]\n",
        "    xs, hs, os, probs = {}, {}, {}, {}\n",
        "    xs[0] = np.zeros((vocab_size, 1))\n",
        "    loss = 0\n",
        "    hs[-1] = np.copy(h_prev)\n",
        "    for t in range(len(x)):\n",
        "        if t > 0:\n",
        "            xs[t] = np.zeros((vocab_size, 1))\n",
        "            xs[t][x[t]] = 1\n",
        "        hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t - 1]) + b)\n",
        "        os[t] = np.dot(Why, hs[t]) + c\n",
        "        probs[t] = softmax(os[t])\n",
        "        loss -= np.log(probs[t][y[t], 0])\n",
        "    cache = (xs, hs, probs)\n",
        "    return loss, cache"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def clip_gradients(gradients, max_value):\n",
        "    for grad in gradients.keys():\n",
        "        np.clip(gradients[grad], -max_value, max_value, out=gradients[grad])\n",
        "    return gradients\n",
        "\n",
        "def rnn_backward(y, parameters, cache):\n",
        "    # Retrieve xs, hs, and probs\n",
        "    xs, hs, probs = cache\n",
        "    \n",
        "    # Initialize all gradients to zero\n",
        "    dh_next = np.zeros_like(hs[0])\n",
        "    parameters_names = [\"Whh\", \"Wxh\", \"b\", \"Why\", \"c\"]\n",
        "    grads = {}\n",
        "    for param_name in parameters_names:\n",
        "        grads[\"d\" + param_name] = np.zeros_like(parameters[param_name])\n",
        "    \n",
        "    # Iterate over all time steps in reverse order starting from Tx\n",
        "    for t in reversed(range(len(xs))):\n",
        "        dy = np.copy(probs[t])\n",
        "        dy[y[t]] -= 1\n",
        "        grads[\"dWhy\"] += np.dot(dy, hs[t].T)\n",
        "        grads[\"dc\"] += dy\n",
        "        dh = np.dot(parameters[\"Why\"].T, dy) + dh_next\n",
        "        dhraw = (1 - hs[t] ** 2) * dh\n",
        "        grads[\"dWhh\"] += np.dot(dhraw, hs[t - 1].T)\n",
        "        grads[\"dWxh\"] += np.dot(dhraw, xs[t].T)\n",
        "        grads[\"db\"] += dhraw\n",
        "        dh_next = np.dot(parameters[\"Whh\"].T, dhraw)\n",
        "        # Clip the gradients using [-5, 5] as the interval\n",
        "        grads = clip_gradients(grads, 5)\n",
        "    \n",
        "    # Get the last hidden state\n",
        "    h_prev = hs[len(xs) - 1]\n",
        "    return grads, h_prev"
      ],
      "metadata": {
        "id": "fkotAXNTUbtm"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sample(parameters, idx_to_chars, chars_to_idx, n):\n",
        "    # Retrienve parameters, shapes, and vocab size\n",
        "    Whh, Wxh, b = parameters[\"Whh\"], parameters[\"Wxh\"], parameters[\"b\"]\n",
        "    Why, c = parameters[\"Why\"], parameters[\"c\"]\n",
        "    n_h, n_x = Wxh.shape\n",
        "    vocab_size = c.shape[0]\n",
        "    \n",
        "    # Initialize a0 and x1 to zero vectors\n",
        "    h_prev = np.zeros((n_h, 1))\n",
        "    x = np.zeros((n_x, 1))\n",
        "    # Initialize empty sequence\n",
        "    indices = []\n",
        "    idx = -1\n",
        "    counter = 0\n",
        "    while (counter <= n and idx != chars_to_idx[\"\\n\"]):\n",
        "        # Fwd propagation\n",
        "        h = np.tanh(np.dot(Whh, h_prev) + np.dot(Wxh, x) + b)\n",
        "        o = np.dot(Why, h) + c\n",
        "        probs = softmax(o)\n",
        "        \n",
        "        # Sample the index of the character using generated probs distribution\n",
        "        idx = np.random.choice(vocab_size, p=probs.ravel())\n",
        "        \n",
        "        # Get the character of the sampled index\n",
        "        char = idx_to_chars[idx]\n",
        "    \n",
        "        # Add the char to the sequence\n",
        "        indices.append(idx)\n",
        "        \n",
        "        # Update a_prev and x\n",
        "        h_prev = np.copy(h)\n",
        "        x = np.zeros((n_x, 1))\n",
        "        x[idx] = 1\n",
        "        \n",
        "        counter += 1\n",
        "    sequence = \"\".join([idx_to_chars[idx] for idx in indices if idx != 0])\n",
        "    return sequence"
      ],
      "metadata": {
        "id": "pBTlkkRDUKzI"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model(\n",
        "        file_path, chars_to_idx, idx_to_chars, hidden_layer_size, vocab_size,\n",
        "        num_epochs=10, learning_rate=0.01):\n",
        "    # Get the data\n",
        "    with open(file_path) as f:\n",
        "        data = f.readlines()\n",
        "    examples = [x.lower().strip() for x in data]\n",
        "\n",
        "    # Initialize parameters\n",
        "    parameters = initialize_parameters(vocab_size, hidden_layer_size)\n",
        "\n",
        "    # Initialize Adam parameters\n",
        "    s = initialize_rmsprop(parameters)\n",
        "\n",
        "    # Initialize loss\n",
        "    smoothed_loss = -np.log(1 / vocab_size) * 7\n",
        "\n",
        "    # Initialize hidden state h0 and overall loss\n",
        "    h_prev = np.zeros((hidden_layer_size, 1))\n",
        "    overall_loss = []\n",
        "\n",
        "    # Iterate over number of epochs\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"\\033[1m\\033[94mEpoch {epoch}\")\n",
        "        print(f\"\\033[1m\\033[92m=======\")\n",
        "\n",
        "        # Sample one name\n",
        "        print(f\"\"\"Sampled name: {sample(parameters, idx_to_chars, chars_to_idx,\n",
        "            10).capitalize()}\"\"\")\n",
        "        print(f\"Smoothed loss: {smoothed_loss:.4f}\\n\")\n",
        "\n",
        "        # Shuffle examples\n",
        "        np.random.shuffle(examples)\n",
        "\n",
        "        # Iterate over all examples (SGD)\n",
        "        for example in examples:\n",
        "            x = [None] + [chars_to_idx[char] for char in example]\n",
        "            y = x[1:] + [chars_to_idx[\"\\n\"]]\n",
        "            # Fwd pass\n",
        "            loss, cache = rnn_forward(x, y, h_prev, parameters)\n",
        "            # Compute smooth loss\n",
        "            smoothed_loss = smooth_loss(smoothed_loss, loss)\n",
        "            # Bwd pass\n",
        "            grads, h_prev = rnn_backward(y, parameters, cache)\n",
        "            # Update parameters\n",
        "            parameters, s = update_parameters_with_rmsprop(\n",
        "                parameters, grads, s)\n",
        "\n",
        "        overall_loss.append(smoothed_loss)\n",
        "    return parameters, overall_loss"
      ],
      "metadata": {
        "id": "3x02sBVSUlc9"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load names\n",
        "data = open(\"rnn.txt\", \"r\").read()\n",
        "\n",
        "# Convert characters to lower case\n",
        "data = data.lower()\n",
        "\n",
        "# Construct vocabulary using unique characters, sort it in ascending order,\n",
        "# then construct two dictionaries that maps character to index and index to\n",
        "# characters.\n",
        "chars = list(sorted(set(data)))\n",
        "chars_to_idx = {ch:i for i, ch in enumerate(chars)}\n",
        "idx_to_chars = {i:ch for ch, i in chars_to_idx.items()}\n",
        "\n",
        "# Get the size of the data and vocab size\n",
        "data_size = len(data)\n",
        "vocab_size = len(chars_to_idx)\n",
        "print(f\"There are {data_size} characters and {vocab_size} unique characters.\")\n",
        "\n",
        "# Fitting the model\n",
        "parameters, loss = model(\"/content/rnn.txt\", chars_to_idx, idx_to_chars, 100, vocab_size, 10, 0.01)\n",
        "\n",
        "# Plotting the loss\n",
        "plt.plot(range(len(loss)), loss)\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Smoothed loss\");"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-M6gQEBmUqoa",
        "outputId": "dca172a4-3c4e-47f0-9215-cf377bb6724b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 412 characters and 29 unique characters.\n",
            "\u001b[1m\u001b[94mEpoch 0\n",
            "\u001b[1m\u001b[92m=======\n",
            "Sampled name: Lmn,fqgibdm\n",
            "Smoothed loss: 23.5711\n",
            "\n",
            "\u001b[1m\u001b[94mEpoch 1\n",
            "\u001b[1m\u001b[92m=======\n",
            "Sampled name: Hte nc k'yf\n",
            "Smoothed loss: 24.4428\n",
            "\n",
            "\u001b[1m\u001b[94mEpoch 2\n",
            "\u001b[1m\u001b[92m=======\n",
            "Sampled name: Gdsc,sudpe \n",
            "Smoothed loss: 25.2190\n",
            "\n",
            "\u001b[1m\u001b[94mEpoch 3\n",
            "\u001b[1m\u001b[92m=======\n",
            "Sampled name: Ntturionh b\n",
            "Smoothed loss: 25.9805\n",
            "\n",
            "\u001b[1m\u001b[94mEpoch 4\n",
            "\u001b[1m\u001b[92m=======\n",
            "Sampled name: Hdnesf,aiee\n",
            "Smoothed loss: 26.7232\n",
            "\n",
            "\u001b[1m\u001b[94mEpoch 5\n",
            "\u001b[1m\u001b[92m=======\n",
            "Sampled name: Ty srftep,u\n",
            "Smoothed loss: 27.4498\n",
            "\n",
            "\u001b[1m\u001b[94mEpoch 6\n",
            "\u001b[1m\u001b[92m=======\n",
            "Sampled name: ,c linywino\n",
            "Smoothed loss: 28.1591\n",
            "\n",
            "\u001b[1m\u001b[94mEpoch 7\n",
            "\u001b[1m\u001b[92m=======\n",
            "Sampled name: Oru\n",
            "Smoothed loss: 28.8464\n",
            "\n",
            "\u001b[1m\u001b[94mEpoch 8\n",
            "\u001b[1m\u001b[92m=======\n",
            "Sampled name: \n",
            "Smoothed loss: 29.5121\n",
            "\n",
            "\u001b[1m\u001b[94mEpoch 9\n",
            "\u001b[1m\u001b[92m=======\n",
            "Sampled name: Aew icithts\n",
            "Smoothed loss: 30.1523\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KUy1yQCAUumF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}