{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ptAPVAOTQ27J"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import string\n",
        "from nltk.corpus import stopwords"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "stopwods=nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QeWkfb3OQ_EE",
        "outputId": "6d50e200-823a-4a13-d9c1-107b944fcfab"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(x):\n",
        "    e_x = np.exp(x - np.max(x))\n",
        "    return e_x / e_x.sum()"
      ],
      "metadata": {
        "id": "ciy2KJ31RB3w"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class word2vec(object):\n",
        "\tdef __init__(self):\n",
        "\t\tself.N = 10\n",
        "\t\tself.X_train = []\n",
        "\t\tself.y_train = []\n",
        "\t\tself.window_size = 2\n",
        "\t\tself.alpha = 0.001\n",
        "\t\tself.words = []\n",
        "\t\tself.word_index = {}\n",
        "\n",
        "\tdef initialize(self,V,data):\n",
        "\t\tself.V = V\n",
        "\t\tself.W = np.random.uniform(-0.8, 0.8, (self.V, self.N))\n",
        "\t\tself.W1 = np.random.uniform(-0.8, 0.8, (self.N, self.V))\n",
        "\t\t\n",
        "\t\tself.words = data\n",
        "\t\tfor i in range(len(data)):\n",
        "\t\t\tself.word_index[data[i]] = i\n",
        "\n",
        "\tdef feed_forward(self,X):\n",
        "\t\tself.h = np.dot(self.W.T,X).reshape(self.N,1)\n",
        "\t\tself.u = np.dot(self.W1.T,self.h)\n",
        "\t\t#print(self.u)\n",
        "\t\tself.y = softmax(self.u)\n",
        "\t\treturn self.y\n",
        "\t\t\n",
        "\tdef backpropagate(self,x,t):\n",
        "\t\te = self.y - np.asarray(t).reshape(self.V,1)\n",
        "\t\tdLdW1 = np.dot(self.h,e.T)\n",
        "\t\tX = np.array(x).reshape(self.V,1)\n",
        "\t\tdLdW = np.dot(X, np.dot(self.W1,e).T)\n",
        "\t\tself.W1 = self.W1 - self.alpha*dLdW1\n",
        "\t\tself.W = self.W - self.alpha*dLdW\n",
        "\t\t\n",
        "\tdef train(self,epochs):\n",
        "\t\tfor x in range(1,epochs):\t\n",
        "\t\t\tself.loss = 0\n",
        "\t\t\tfor j in range(len(self.X_train)):\n",
        "\t\t\t\tself.feed_forward(self.X_train[j])\n",
        "\t\t\t\tself.backpropagate(self.X_train[j],self.y_train[j])\n",
        "\t\t\t\tC = 0\n",
        "\t\t\t\tfor m in range(self.V):\n",
        "\t\t\t\t\tif(self.y_train[j][m]):\n",
        "\t\t\t\t\t\tself.loss += -1*self.u[m][0]\n",
        "\t\t\t\t\t\tC += 1\n",
        "\t\t\t\tself.loss += C*np.log(np.sum(np.exp(self.u)))\n",
        "\t\t\tprint(\"epoch \",x, \" loss = \",self.loss)\n",
        "\t\t\tself.alpha *= 1/( (1+self.alpha*x) )\n",
        "\t\t\t\n",
        "\tdef predict(self,word,number_of_predictions):\n",
        "\t\tif word in self.words:\n",
        "\t\t\tindex = self.word_index[word]\n",
        "\t\t\tX = [0 for i in range(self.V)]\n",
        "\t\t\tX[index] = 1\n",
        "\t\t\tprediction = self.feed_forward(X)\n",
        "\t\t\toutput = {}\n",
        "\t\t\tfor i in range(self.V):\n",
        "\t\t\t\toutput[prediction[i][0]] = i\n",
        "\t\t\t\n",
        "\t\t\ttop_context_words = []\n",
        "\t\t\tfor k in sorted(output,reverse=True):\n",
        "\t\t\t\ttop_context_words.append(self.words[output[k]])\n",
        "\t\t\t\tif(len(top_context_words)>=number_of_predictions):\n",
        "\t\t\t\t\tbreak\n",
        "\t\n",
        "\t\t\treturn top_context_words\n",
        "\t\telse:\n",
        "\t\t\tprint(\"Word not found in dictionary\")"
      ],
      "metadata": {
        "id": "uyK-sfdNRELt"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocessing(corpus):\n",
        "    stop_words = set(stopwords.words('english'))   \n",
        "    training_data = []\n",
        "    sentences = corpus.split(\".\")\n",
        "    for i in range(len(sentences)):\n",
        "        sentences[i] = sentences[i].strip()\n",
        "        sentence = sentences[i].split()\n",
        "        x = [word.strip(string.punctuation) for word in sentence\n",
        "                                     if word not in stop_words]\n",
        "        x = [word.lower() for word in x]\n",
        "        training_data.append(x)\n",
        "    return training_data"
      ],
      "metadata": {
        "id": "5eglT1fFRGkT"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_data_for_training(sentences,w2v):\n",
        "    data = {}\n",
        "    for sentence in sentences:\n",
        "        for word in sentence:\n",
        "            if word not in data:\n",
        "                data[word] = 1\n",
        "            else:\n",
        "                data[word] += 1\n",
        "    V = len(data)\n",
        "    data = sorted(list(data.keys()))\n",
        "    vocab = {}\n",
        "    for i in range(len(data)):\n",
        "        vocab[data[i]] = i\n",
        "        \n",
        "    for sentence in sentences:\n",
        "        for i in range(len(sentence)):\n",
        "            center_word = [0 for x in range(V)]\n",
        "            center_word[vocab[sentence[i]]] = 1\n",
        "            context = [0 for x in range(V)]\n",
        "             \n",
        "            for j in range(i-w2v.window_size,i+w2v.window_size):\n",
        "                if i!=j and j>=0 and j<len(sentence):\n",
        "                    context[vocab[sentence[j]]] += 1\n",
        "            w2v.X_train.append(center_word)\n",
        "            w2v.y_train.append(context)\n",
        "    w2v.initialize(V,data)\n",
        "  \n",
        "    return w2v.X_train,w2v.y_train"
      ],
      "metadata": {
        "id": "V2ra9VmERKnn"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = \"\"\n",
        "corpus += \"The quick brown fox jumps over the lazy dog\"\n",
        "epochs = 500\n",
        "\n",
        "training_data = preprocessing(corpus)\n",
        "w2v = word2vec()\n",
        "\n",
        "prepare_data_for_training(training_data,w2v)\n",
        "w2v.train(epochs)\n",
        "\n",
        "print(w2v.predict(\"jumps\",5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QUF3INahRNEe",
        "outputId": "4d9604d5-7f23-4d7b-ba4b-f7996a0b2f33"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch  1  loss =  35.12355865498064\n",
            "epoch  2  loss =  35.07516221917024\n",
            "epoch  3  loss =  35.02698713104852\n",
            "epoch  4  loss =  34.97907970607912\n",
            "epoch  5  loss =  34.93148502379144\n",
            "epoch  6  loss =  34.88424667784625\n",
            "epoch  7  loss =  34.83740654525754\n",
            "epoch  8  loss =  34.79100457768517\n",
            "epoch  9  loss =  34.74507861717439\n",
            "epoch  10  loss =  34.69966423813613\n",
            "epoch  11  loss =  34.65479461675882\n",
            "epoch  12  loss =  34.6105004284391\n",
            "epoch  13  loss =  34.56680977323654\n",
            "epoch  14  loss =  34.523748128813246\n",
            "epoch  15  loss =  34.48133832982882\n",
            "epoch  16  loss =  34.439600572335046\n",
            "epoch  17  loss =  34.39855244136147\n",
            "epoch  18  loss =  34.35820895960651\n",
            "epoch  19  loss =  34.3185826549489\n",
            "epoch  20  loss =  34.27968364436937\n",
            "epoch  21  loss =  34.24151973181817\n",
            "epoch  22  loss =  34.204096517571\n",
            "epoch  23  loss =  34.1674175166799\n",
            "epoch  24  loss =  34.131484284232904\n",
            "epoch  25  loss =  34.09629654528233\n",
            "epoch  26  loss =  34.06185232747287\n",
            "epoch  27  loss =  34.028148094592275\n",
            "epoch  28  loss =  33.99517887946846\n",
            "epoch  29  loss =  33.96293841484341\n",
            "epoch  30  loss =  33.93141926105703\n",
            "epoch  31  loss =  33.900612929571814\n",
            "epoch  32  loss =  33.87051000155546\n",
            "epoch  33  loss =  33.84110024091182\n",
            "epoch  34  loss =  33.81237270130906\n",
            "epoch  35  loss =  33.78431582689583\n",
            "epoch  36  loss =  33.75691754652173\n",
            "epoch  37  loss =  33.73016536138753\n",
            "epoch  38  loss =  33.7040464261433\n",
            "epoch  39  loss =  33.67854762353113\n",
            "epoch  40  loss =  33.65365563273221\n",
            "epoch  41  loss =  33.62935699163\n",
            "epoch  42  loss =  33.60563815324011\n",
            "epoch  43  loss =  33.58248553658718\n",
            "epoch  44  loss =  33.559885572329584\n",
            "epoch  45  loss =  33.537824743445185\n",
            "epoch  46  loss =  33.516289621298185\n",
            "epoch  47  loss =  33.495266897407475\n",
            "epoch  48  loss =  33.47474341123374\n",
            "epoch  49  loss =  33.4547061742949\n",
            "epoch  50  loss =  33.435142390910315\n",
            "epoch  51  loss =  33.41603947586117\n",
            "epoch  52  loss =  33.397385069241544\n",
            "epoch  53  loss =  33.37916704875976\n",
            "epoch  54  loss =  33.36137353973415\n",
            "epoch  55  loss =  33.34399292301221\n",
            "epoch  56  loss =  33.327013841025746\n",
            "epoch  57  loss =  33.31042520218025\n",
            "epoch  58  loss =  33.29421618376065\n",
            "epoch  59  loss =  33.2783762335218\n",
            "epoch  60  loss =  33.26289507011787\n",
            "epoch  61  loss =  33.247762682511755\n",
            "epoch  62  loss =  33.23296932849288\n",
            "epoch  63  loss =  33.218505532420345\n",
            "epoch  64  loss =  33.204362082297315\n",
            "epoch  65  loss =  33.19053002627216\n",
            "epoch  66  loss =  33.17700066865277\n",
            "epoch  67  loss =  33.16376556551137\n",
            "epoch  68  loss =  33.15081651994932\n",
            "epoch  69  loss =  33.13814557708406\n",
            "epoch  70  loss =  33.12574501881327\n",
            "epoch  71  loss =  33.11360735840577\n",
            "epoch  72  loss =  33.10172533496231\n",
            "epoch  73  loss =  33.090091907784895\n",
            "epoch  74  loss =  33.07870025068854\n",
            "epoch  75  loss =  33.06754374628483\n",
            "epoch  76  loss =  33.05661598026342\n",
            "epoch  77  loss =  33.04591073569388\n",
            "epoch  78  loss =  33.03542198736714\n",
            "epoch  79  loss =  33.025143896193526\n",
            "epoch  80  loss =  33.01507080367145\n",
            "epoch  81  loss =  33.00519722643869\n",
            "epoch  82  loss =  32.99551785091674\n",
            "epoch  83  loss =  32.98602752805623\n",
            "epoch  84  loss =  32.976721268190374\n",
            "epoch  85  loss =  32.967594236002014\n",
            "epoch  86  loss =  32.95864174560849\n",
            "epoch  87  loss =  32.94985925576731\n",
            "epoch  88  loss =  32.94124236520535\n",
            "epoch  89  loss =  32.93278680807257\n",
            "epoch  90  loss =  32.92448844952143\n",
            "epoch  91  loss =  32.91634328141167\n",
            "epoch  92  loss =  32.90834741814065\n",
            "epoch  93  loss =  32.90049709259779\n",
            "epoch  94  loss =  32.89278865224234\n",
            "epoch  95  loss =  32.88521855530256\n",
            "epoch  96  loss =  32.87778336709466\n",
            "epoch  97  loss =  32.87047975645936\n",
            "epoch  98  loss =  32.86330449231374\n",
            "epoch  99  loss =  32.85625444031582\n",
            "epoch  100  loss =  32.84932655963966\n",
            "epoch  101  loss =  32.84251789985784\n",
            "epoch  102  loss =  32.83582559792901\n",
            "epoch  103  loss =  32.82924687528732\n",
            "epoch  104  loss =  32.822779035031296\n",
            "epoch  105  loss =  32.81641945920906\n",
            "epoch  106  loss =  32.81016560619703\n",
            "epoch  107  loss =  32.804015008169586\n",
            "epoch  108  loss =  32.79796526865647\n",
            "epoch  109  loss =  32.79201406018545\n",
            "epoch  110  loss =  32.78615912200746\n",
            "epoch  111  loss =  32.78039825790132\n",
            "epoch  112  loss =  32.77472933405578\n",
            "epoch  113  loss =  32.76915027702579\n",
            "epoch  114  loss =  32.76365907176099\n",
            "epoch  115  loss =  32.75825375970354\n",
            "epoch  116  loss =  32.752932436953095\n",
            "epoch  117  loss =  32.7476932524965\n",
            "epoch  118  loss =  32.74253440649998\n",
            "epoch  119  loss =  32.73745414866156\n",
            "epoch  120  loss =  32.7324507766215\n",
            "epoch  121  loss =  32.72752263442892\n",
            "epoch  122  loss =  32.7226681110622\n",
            "epoch  123  loss =  32.71788563900156\n",
            "epoch  124  loss =  32.713173692851775\n",
            "epoch  125  loss =  32.70853078801312\n",
            "epoch  126  loss =  32.70395547939892\n",
            "epoch  127  loss =  32.69944636019802\n",
            "epoch  128  loss =  32.69500206068023\n",
            "epoch  129  loss =  32.69062124704368\n",
            "epoch  130  loss =  32.68630262030196\n",
            "epoch  131  loss =  32.682044915210085\n",
            "epoch  132  loss =  32.677846899227504\n",
            "epoch  133  loss =  32.673707371516905\n",
            "epoch  134  loss =  32.669625161977635\n",
            "epoch  135  loss =  32.665599130312145\n",
            "epoch  136  loss =  32.66162816512457\n",
            "epoch  137  loss =  32.657711183050004\n",
            "epoch  138  loss =  32.65384712791353\n",
            "epoch  139  loss =  32.65003496991769\n",
            "epoch  140  loss =  32.64627370485768\n",
            "epoch  141  loss =  32.64256235336285\n",
            "epoch  142  loss =  32.638899960163805\n",
            "epoch  143  loss =  32.635285593384225\n",
            "epoch  144  loss =  32.631718343856164\n",
            "epoch  145  loss =  32.62819732445836\n",
            "epoch  146  loss =  32.62472166947646\n",
            "epoch  147  loss =  32.62129053398443\n",
            "epoch  148  loss =  32.61790309324645\n",
            "epoch  149  loss =  32.61455854213842\n",
            "epoch  150  loss =  32.61125609458853\n",
            "epoch  151  loss =  32.60799498303599\n",
            "epoch  152  loss =  32.60477445790745\n",
            "epoch  153  loss =  32.601593787110495\n",
            "epoch  154  loss =  32.59845225554336\n",
            "epoch  155  loss =  32.5953491646205\n",
            "epoch  156  loss =  32.59228383181349\n",
            "epoch  157  loss =  32.589255590206484\n",
            "epoch  158  loss =  32.58626378806587\n",
            "epoch  159  loss =  32.583307788423646\n",
            "epoch  160  loss =  32.58038696867389\n",
            "epoch  161  loss =  32.57750072018209\n",
            "epoch  162  loss =  32.57464844790651\n",
            "epoch  163  loss =  32.57182957003169\n",
            "epoch  164  loss =  32.56904351761311\n",
            "epoch  165  loss =  32.56628973423302\n",
            "epoch  166  loss =  32.56356767566698\n",
            "epoch  167  loss =  32.56087680956045\n",
            "epoch  168  loss =  32.55821661511555\n",
            "epoch  169  loss =  32.555586582787264\n",
            "epoch  170  loss =  32.552986213988945\n",
            "epoch  171  loss =  32.55041502080672\n",
            "epoch  172  loss =  32.547872525722575\n",
            "epoch  173  loss =  32.545358261345754\n",
            "epoch  174  loss =  32.542871770152225\n",
            "epoch  175  loss =  32.54041260423193\n",
            "epoch  176  loss =  32.53798032504357\n",
            "epoch  177  loss =  32.53557450317658\n",
            "epoch  178  loss =  32.53319471812032\n",
            "epoch  179  loss =  32.53084055803987\n",
            "epoch  180  loss =  32.52851161955847\n",
            "epoch  181  loss =  32.52620750754636\n",
            "epoch  182  loss =  32.52392783491566\n",
            "epoch  183  loss =  32.52167222242129\n",
            "epoch  184  loss =  32.519440298467586\n",
            "epoch  185  loss =  32.51723169892048\n",
            "epoch  186  loss =  32.51504606692509\n",
            "epoch  187  loss =  32.51288305272844\n",
            "epoch  188  loss =  32.510742313507315\n",
            "epoch  189  loss =  32.50862351320093\n",
            "epoch  190  loss =  32.50652632234832\n",
            "epoch  191  loss =  32.5044504179303\n",
            "epoch  192  loss =  32.502395483215906\n",
            "epoch  193  loss =  32.50036120761301\n",
            "epoch  194  loss =  32.49834728652317\n",
            "epoch  195  loss =  32.49635342120046\n",
            "epoch  196  loss =  32.4943793186142\n",
            "epoch  197  loss =  32.49242469131543\n",
            "epoch  198  loss =  32.49048925730711\n",
            "epoch  199  loss =  32.48857273991773\n",
            "epoch  200  loss =  32.48667486767849\n",
            "epoch  201  loss =  32.48479537420366\n",
            "epoch  202  loss =  32.48293399807431\n",
            "epoch  203  loss =  32.48109048272505\n",
            "epoch  204  loss =  32.4792645763338\n",
            "epoch  205  loss =  32.47745603171458\n",
            "epoch  206  loss =  32.47566460621309\n",
            "epoch  207  loss =  32.473890061604976\n",
            "epoch  208  loss =  32.472132163996946\n",
            "epoch  209  loss =  32.470390683730315\n",
            "epoch  210  loss =  32.468665395287154\n",
            "epoch  211  loss =  32.46695607719888\n",
            "epoch  212  loss =  32.46526251195718\n",
            "epoch  213  loss =  32.46358448592726\n",
            "epoch  214  loss =  32.46192178926334\n",
            "epoch  215  loss =  32.46027421582632\n",
            "epoch  216  loss =  32.458641563103434\n",
            "epoch  217  loss =  32.457023632130195\n",
            "epoch  218  loss =  32.45542022741401\n",
            "epoch  219  loss =  32.45383115686001\n",
            "epoch  220  loss =  32.452256231698556\n",
            "epoch  221  loss =  32.45069526641466\n",
            "epoch  222  loss =  32.44914807867911\n",
            "epoch  223  loss =  32.44761448928138\n",
            "epoch  224  loss =  32.4460943220641\n",
            "epoch  225  loss =  32.44458740385928\n",
            "epoch  226  loss =  32.44309356442597\n",
            "epoch  227  loss =  32.441612636389564\n",
            "epoch  228  loss =  32.4401444551825\n",
            "epoch  229  loss =  32.43868885898645\n",
            "epoch  230  loss =  32.43724568867592\n",
            "epoch  231  loss =  32.43581478776318\n",
            "epoch  232  loss =  32.43439600234454\n",
            "epoch  233  loss =  32.43298918104793\n",
            "epoch  234  loss =  32.4315941749817\n",
            "epoch  235  loss =  32.43021083768466\n",
            "epoch  236  loss =  32.42883902507736\n",
            "epoch  237  loss =  32.42747859541436\n",
            "epoch  238  loss =  32.426129409237845\n",
            "epoch  239  loss =  32.42479132933215\n",
            "epoch  240  loss =  32.4234642206795\n",
            "epoch  241  loss =  32.42214795041656\n",
            "epoch  242  loss =  32.420842387792334\n",
            "epoch  243  loss =  32.41954740412667\n",
            "epoch  244  loss =  32.41826287277\n",
            "epoch  245  loss =  32.41698866906386\n",
            "epoch  246  loss =  32.415724670302396\n",
            "epoch  247  loss =  32.41447075569469\n",
            "epoch  248  loss =  32.41322680632798\n",
            "epoch  249  loss =  32.41199270513166\n",
            "epoch  250  loss =  32.41076833684224\n",
            "epoch  251  loss =  32.40955358796887\n",
            "epoch  252  loss =  32.40834834675986\n",
            "epoch  253  loss =  32.40715250316978\n",
            "epoch  254  loss =  32.405965948827436\n",
            "epoch  255  loss =  32.40478857700441\n",
            "epoch  256  loss =  32.403620282584455\n",
            "epoch  257  loss =  32.40246096203341\n",
            "epoch  258  loss =  32.40131051336994\n",
            "epoch  259  loss =  32.400168836136736\n",
            "epoch  260  loss =  32.39903583137249\n",
            "epoch  261  loss =  32.39791140158446\n",
            "epoch  262  loss =  32.39679545072153\n",
            "epoch  263  loss =  32.395687884147996\n",
            "epoch  264  loss =  32.394588608617816\n",
            "epoch  265  loss =  32.393497532249405\n",
            "epoch  266  loss =  32.39241456450107\n",
            "epoch  267  loss =  32.39133961614692\n",
            "epoch  268  loss =  32.39027259925317\n",
            "epoch  269  loss =  32.389213427155205\n",
            "epoch  270  loss =  32.388162014434855\n",
            "epoch  271  loss =  32.38711827689836\n",
            "epoch  272  loss =  32.386082131554666\n",
            "epoch  273  loss =  32.38505349659423\n",
            "epoch  274  loss =  32.38403229136826\n",
            "epoch  275  loss =  32.38301843636842\n",
            "epoch  276  loss =  32.38201185320686\n",
            "epoch  277  loss =  32.38101246459678\n",
            "epoch  278  loss =  32.380020194333305\n",
            "epoch  279  loss =  32.3790349672748\n",
            "epoch  280  loss =  32.378056709324525\n",
            "epoch  281  loss =  32.37708534741274\n",
            "epoch  282  loss =  32.37612080947907\n",
            "epoch  283  loss =  32.37516302445534\n",
            "epoch  284  loss =  32.374211922248676\n",
            "epoch  285  loss =  32.37326743372493\n",
            "epoch  286  loss =  32.372329490692564\n",
            "epoch  287  loss =  32.3713980258867\n",
            "epoch  288  loss =  32.37047297295359\n",
            "epoch  289  loss =  32.369554266435394\n",
            "epoch  290  loss =  32.36864184175517\n",
            "epoch  291  loss =  32.36773563520228\n",
            "epoch  292  loss =  32.36683558391801\n",
            "epoch  293  loss =  32.36594162588147\n",
            "epoch  294  loss =  32.365053699895824\n",
            "epoch  295  loss =  32.364171745574716\n",
            "epoch  296  loss =  32.36329570332906\n",
            "epoch  297  loss =  32.362425514353966\n",
            "epoch  298  loss =  32.361561120616\n",
            "epoch  299  loss =  32.36070246484073\n",
            "epoch  300  loss =  32.35984949050032\n",
            "epoch  301  loss =  32.359002141801625\n",
            "epoch  302  loss =  32.358160363674315\n",
            "epoch  303  loss =  32.35732410175934\n",
            "epoch  304  loss =  32.35649330239748\n",
            "epoch  305  loss =  32.355667912618316\n",
            "epoch  306  loss =  32.35484788012918\n",
            "epoch  307  loss =  32.35403315330455\n",
            "epoch  308  loss =  32.35322368117539\n",
            "epoch  309  loss =  32.352419413418936\n",
            "epoch  310  loss =  32.351620300348465\n",
            "epoch  311  loss =  32.35082629290342\n",
            "epoch  312  loss =  32.350037342639595\n",
            "epoch  313  loss =  32.3492534017196\n",
            "epoch  314  loss =  32.34847442290342\n",
            "epoch  315  loss =  32.347700359539175\n",
            "epoch  316  loss =  32.34693116555408\n",
            "epoch  317  loss =  32.34616679544555\n",
            "epoch  318  loss =  32.345407204272455\n",
            "epoch  319  loss =  32.34465234764654\n",
            "epoch  320  loss =  32.34390218172401\n",
            "epoch  321  loss =  32.34315666319729\n",
            "epoch  322  loss =  32.34241574928683\n",
            "epoch  323  loss =  32.34167939773325\n",
            "epoch  324  loss =  32.340947566789374\n",
            "epoch  325  loss =  32.34022021521264\n",
            "epoch  326  loss =  32.33949730225754\n",
            "epoch  327  loss =  32.33877878766815\n",
            "epoch  328  loss =  32.338064631670875\n",
            "epoch  329  loss =  32.337354794967325\n",
            "epoch  330  loss =  32.336649238727205\n",
            "epoch  331  loss =  32.335947924581475\n",
            "epoch  332  loss =  32.33525081461557\n",
            "epoch  333  loss =  32.33455787136263\n",
            "epoch  334  loss =  32.3338690577971\n",
            "epoch  335  loss =  32.333184337328156\n",
            "epoch  336  loss =  32.3325036737935\n",
            "epoch  337  loss =  32.33182703145305\n",
            "epoch  338  loss =  32.33115437498284\n",
            "epoch  339  loss =  32.33048566946908\n",
            "epoch  340  loss =  32.329820880402224\n",
            "epoch  341  loss =  32.32915997367113\n",
            "epoch  342  loss =  32.3285029155574\n",
            "epoch  343  loss =  32.32784967272981\n",
            "epoch  344  loss =  32.32720021223869\n",
            "epoch  345  loss =  32.32655450151062\n",
            "epoch  346  loss =  32.32591250834307\n",
            "epoch  347  loss =  32.325274200899166\n",
            "epoch  348  loss =  32.32463954770251\n",
            "epoch  349  loss =  32.324008517632166\n",
            "epoch  350  loss =  32.32338107991767\n",
            "epoch  351  loss =  32.32275720413411\n",
            "epoch  352  loss =  32.322136860197396\n",
            "epoch  353  loss =  32.3215200183594\n",
            "epoch  354  loss =  32.32090664920341\n",
            "epoch  355  loss =  32.32029672363954\n",
            "epoch  356  loss =  32.31969021290014\n",
            "epoch  357  loss =  32.319087088535504\n",
            "epoch  358  loss =  32.3184873224094\n",
            "epoch  359  loss =  32.31789088669485\n",
            "epoch  360  loss =  32.317297753869866\n",
            "epoch  361  loss =  32.31670789671337\n",
            "epoch  362  loss =  32.31612128830105\n",
            "epoch  363  loss =  32.315537902001395\n",
            "epoch  364  loss =  32.31495771147172\n",
            "epoch  365  loss =  32.31438069065426\n",
            "epoch  366  loss =  32.31380681377241\n",
            "epoch  367  loss =  32.313236055326904\n",
            "epoch  368  loss =  32.31266839009215\n",
            "epoch  369  loss =  32.31210379311254\n",
            "epoch  370  loss =  32.31154223969893\n",
            "epoch  371  loss =  32.31098370542508\n",
            "epoch  372  loss =  32.310428166124154\n",
            "epoch  373  loss =  32.30987559788532\n",
            "epoch  374  loss =  32.309325977050435\n",
            "epoch  375  loss =  32.30877928021064\n",
            "epoch  376  loss =  32.30823548420318\n",
            "epoch  377  loss =  32.30769456610814\n",
            "epoch  378  loss =  32.307156503245324\n",
            "epoch  379  loss =  32.30662127317114\n",
            "epoch  380  loss =  32.30608885367549\n",
            "epoch  381  loss =  32.30555922277882\n",
            "epoch  382  loss =  32.30503235872911\n",
            "epoch  383  loss =  32.304508239998945\n",
            "epoch  384  loss =  32.30398684528269\n",
            "epoch  385  loss =  32.30346815349359\n",
            "epoch  386  loss =  32.302952143761004\n",
            "epoch  387  loss =  32.302438795427676\n",
            "epoch  388  loss =  32.301928088047006\n",
            "epoch  389  loss =  32.30142000138036\n",
            "epoch  390  loss =  32.30091451539452\n",
            "epoch  391  loss =  32.30041161025899\n",
            "epoch  392  loss =  32.29991126634355\n",
            "epoch  393  loss =  32.29941346421569\n",
            "epoch  394  loss =  32.29891818463813\n",
            "epoch  395  loss =  32.29842540856641\n",
            "epoch  396  loss =  32.297935117146515\n",
            "epoch  397  loss =  32.297447291712444\n",
            "epoch  398  loss =  32.29696191378393\n",
            "epoch  399  loss =  32.29647896506414\n",
            "epoch  400  loss =  32.2959984274374\n",
            "epoch  401  loss =  32.29552028296697\n",
            "epoch  402  loss =  32.295044513892876\n",
            "epoch  403  loss =  32.29457110262969\n",
            "epoch  404  loss =  32.29410003176448\n",
            "epoch  405  loss =  32.29363128405461\n",
            "epoch  406  loss =  32.29316484242578\n",
            "epoch  407  loss =  32.292700689969905\n",
            "epoch  408  loss =  32.292238809943115\n",
            "epoch  409  loss =  32.29177918576383\n",
            "epoch  410  loss =  32.2913218010107\n",
            "epoch  411  loss =  32.29086663942085\n",
            "epoch  412  loss =  32.290413684887774\n",
            "epoch  413  loss =  32.289962921459654\n",
            "epoch  414  loss =  32.28951433333737\n",
            "epoch  415  loss =  32.28906790487279\n",
            "epoch  416  loss =  32.2886236205669\n",
            "epoch  417  loss =  32.288181465068135\n",
            "epoch  418  loss =  32.2877414231705\n",
            "epoch  419  loss =  32.28730347981198\n",
            "epoch  420  loss =  32.28686762007278\n",
            "epoch  421  loss =  32.28643382917366\n",
            "epoch  422  loss =  32.28600209247434\n",
            "epoch  423  loss =  32.28557239547181\n",
            "epoch  424  loss =  32.285144723798766\n",
            "epoch  425  loss =  32.28471906322203\n",
            "epoch  426  loss =  32.284295399640996\n",
            "epoch  427  loss =  32.28387371908609\n",
            "epoch  428  loss =  32.28345400771724\n",
            "epoch  429  loss =  32.28303625182243\n",
            "epoch  430  loss =  32.2826204378162\n",
            "epoch  431  loss =  32.282206552238165\n",
            "epoch  432  loss =  32.28179458175164\n",
            "epoch  433  loss =  32.281384513142186\n",
            "epoch  434  loss =  32.28097633331625\n",
            "epoch  435  loss =  32.28057002929975\n",
            "epoch  436  loss =  32.28016558823678\n",
            "epoch  437  loss =  32.27976299738819\n",
            "epoch  438  loss =  32.279362244130354\n",
            "epoch  439  loss =  32.278963315953796\n",
            "epoch  440  loss =  32.27856620046197\n",
            "epoch  441  loss =  32.27817088536993\n",
            "epoch  442  loss =  32.277777358503144\n",
            "epoch  443  loss =  32.2773856077962\n",
            "epoch  444  loss =  32.27699562129167\n",
            "epoch  445  loss =  32.276607387138775\n",
            "epoch  446  loss =  32.27622089359237\n",
            "epoch  447  loss =  32.27583612901163\n",
            "epoch  448  loss =  32.27545308185903\n",
            "epoch  449  loss =  32.27507174069903\n",
            "epoch  450  loss =  32.274692094197164\n",
            "epoch  451  loss =  32.27431413111875\n",
            "epoch  452  loss =  32.27393784032791\n",
            "epoch  453  loss =  32.27356321078642\n",
            "epoch  454  loss =  32.27319023155272\n",
            "epoch  455  loss =  32.27281889178078\n",
            "epoch  456  loss =  32.27244918071911\n",
            "epoch  457  loss =  32.27208108770972\n",
            "epoch  458  loss =  32.271714602187174\n",
            "epoch  459  loss =  32.27134971367743\n",
            "epoch  460  loss =  32.27098641179704\n",
            "epoch  461  loss =  32.27062468625206\n",
            "epoch  462  loss =  32.27026452683715\n",
            "epoch  463  loss =  32.26990592343458\n",
            "epoch  464  loss =  32.269548866013324\n",
            "epoch  465  loss =  32.26919334462811\n",
            "epoch  466  loss =  32.268839349418556\n",
            "epoch  467  loss =  32.268486870608236\n",
            "epoch  468  loss =  32.26813589850378\n",
            "epoch  469  loss =  32.26778642349402\n",
            "epoch  470  loss =  32.2674384360491\n",
            "epoch  471  loss =  32.26709192671966\n",
            "epoch  472  loss =  32.26674688613595\n",
            "epoch  473  loss =  32.266403305007\n",
            "epoch  474  loss =  32.26606117411985\n",
            "epoch  475  loss =  32.26572048433864\n",
            "epoch  476  loss =  32.265381226603886\n",
            "epoch  477  loss =  32.26504339193167\n",
            "epoch  478  loss =  32.26470697141282\n",
            "epoch  479  loss =  32.26437195621218\n",
            "epoch  480  loss =  32.26403833756785\n",
            "epoch  481  loss =  32.26370610679035\n",
            "epoch  482  loss =  32.263375255261984\n",
            "epoch  483  loss =  32.26304577443604\n",
            "epoch  484  loss =  32.26271765583607\n",
            "epoch  485  loss =  32.26239089105516\n",
            "epoch  486  loss =  32.26206547175528\n",
            "epoch  487  loss =  32.26174138966651\n",
            "epoch  488  loss =  32.261418636586356\n",
            "epoch  489  loss =  32.261097204379155\n",
            "epoch  490  loss =  32.26077708497523\n",
            "epoch  491  loss =  32.26045827037043\n",
            "epoch  492  loss =  32.260140752625276\n",
            "epoch  493  loss =  32.25982452386446\n",
            "epoch  494  loss =  32.25950957627611\n",
            "epoch  495  loss =  32.259195902111145\n",
            "epoch  496  loss =  32.25888349368276\n",
            "epoch  497  loss =  32.25857234336565\n",
            "epoch  498  loss =  32.25826244359553\n",
            "epoch  499  loss =  32.25795378686844\n",
            "['brown', 'jumps', 'lazy', 'fox', 'dog']\n"
          ]
        }
      ]
    }
  ]
}